 The main goal of this project was to alleviate the limited production of Arabic sign 
language recognition projects and improve hearing-impaired individuals' communication 
abilities. Using a deep learning approach to produce a model for a sign language recognition 
system, classify video input of complex gestures into corresponding words or phrases, giving 
accurate real-time translations for the Arabic Sign Language vocabulary. The three proposed 
models were the LSTM model The accuracy of this model was 91% for training data and 85.6% 
for testing. The second model was 1DCNN, and the accuracy for this model was 85.2% for 
training data and 80.1% for testing data. The third proposed model was a hybrid model of 
1DCNN+LSTM, and this model's accuracy was 88% for training data and 83.7% for testing. 
